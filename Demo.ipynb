{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from argparse import ArgumentParser, FileType\n",
    "from importlib import import_module\n",
    "from itertools import count\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import loss\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    excluder='market1501'\n",
    "    query_dataset='/home/k/kajal/triplet-reid/data/market1501/market1501_query.csv'\n",
    "    query_embeddings='/home/k/kajal/triplet-reid/experiments/mkt_recon_ce_exp/query_embeddings.h5'\n",
    "    gallery_dataset='/home/k/kajal/triplet-reid/data/market1501/market1501_test.csv'\n",
    "    gallery_embeddings='/home/k/kajal/triplet-reid/experiments/mkt_recon_ce_exp/gallery_embeddings.h5'\n",
    "    metric='euclidean'\n",
    "    #filename='None'\n",
    "    batch_size=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()\n",
    "args.gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that parameters are set correctly.\n",
    "#args = parser.parse_args()\n",
    "\n",
    "# Load the query and gallery data from the CSV files.\n",
    "query_pids, query_fids = common.load_dataset(args.query_dataset, None)\n",
    "gallery_pids, gallery_fids = common.load_dataset(args.gallery_dataset, None)\n",
    "# Load the two datasets fully into memory.\n",
    "with h5py.File(args.query_embeddings, 'r') as f_query:\n",
    "    query_embs = np.array(f_query['emb'])\n",
    "with h5py.File(args.gallery_embeddings, 'r') as f_gallery:\n",
    "    gallery_embs = np.array(f_gallery['emb'])\n",
    "\n",
    "# Just a quick sanity check that both have the same embedding dimension!\n",
    "query_dim = query_embs.shape[1]\n",
    "gallery_dim = gallery_embs.shape[1]\n",
    "if query_dim != gallery_dim:\n",
    "    raise ValueError('Shape mismatch between query ({}) and gallery ({}) '\n",
    "                     'dimension'.format(query_dim, gallery_dim))\n",
    "\n",
    "# Setup the dataset specific matching function\n",
    "excluder = import_module('excluders.' + args.excluder).Excluder(gallery_fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2fce840a4858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_pids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_fids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_pids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_fids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.batch(args.batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices((query_pids, query_fids, query_embs))#.batch(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-38bbd3148fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-38bbd3148fd1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# We go through the queries in batches, but we always need the whole gallery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices((query_pids, query_fids, query_embs)\n\u001b[0m\u001b[1;32m      6\u001b[0m     ).batch(args.batch_size)#.make_one_shot_iterator().get_next()\n\u001b[1;32m      7\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_pids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_fids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Verify that parameters are set correctly.\n",
    "    \n",
    "    # We go through the queries in batches, but we always need the whole gallery\n",
    "    batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices((query_pids, query_fids, query_embs)\n",
    "    ).batch(args.batch_size)#.make_one_shot_iterator().get_next()\n",
    "    iterator = iter(batch_pids,batch_fids,batch_embs)\n",
    "    next_element = iterator.get_next()\n",
    "    #tf.compat.v1.data.make_one_shot_iterator\n",
    "    batch_distances = loss.cdist(batch_embs, gallery_embs, metric=args.metric)\n",
    "    id_batch_distances = loss.cdist(batch_embs, gallery_embs, 'ce')\n",
    "\n",
    "\n",
    "    # Loop over the query embeddings and compute their APs and the CMC curve.\n",
    "    aps = []\n",
    "    cmc = np.zeros(len(gallery_pids), dtype=np.int32)\n",
    "    acc = np.zeros(len(gallery_pids), dtype=np.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        for start_idx in count(step=args.batch_size):\n",
    "            try:\n",
    "                # Compute distance to all gallery embeddings\n",
    "                distances, dist, pids, fids = sess.run([\n",
    "                    batch_distances, id_batch_distances, batch_pids, batch_fids])\n",
    "                print('\\rEvaluating batch {}-{}/{}'.format(\n",
    "                        start_idx, start_idx + len(fids), len(query_fids)),\n",
    "                      flush=True, end='')\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print()  # Done!\n",
    "                break\n",
    "\n",
    "            # Convert the array of objects back to array of strings\n",
    "            #print(pids)\n",
    "            pids, fids = np.array(pids, '|U'), np.array(fids, '|U')\n",
    "            print(pids)\n",
    "\n",
    "            # Compute the pid matches\n",
    "            pid_matches = gallery_pids[None] == pids[:,None]\n",
    "\n",
    "            # Get a mask indicating True for those gallery entries that should\n",
    "            # be ignored for whatever reason (same camera, junk, ...) and\n",
    "            # exclude those in a way that doesn't affect CMC and mAP.\n",
    "            mask = excluder(fids)\n",
    "            distances[mask] = np.inf\n",
    "            pid_matches[mask] = False\n",
    "\n",
    "            # Keep track of statistics. Invert distances to scores using any\n",
    "            # arbitrary inversion, as long as it's monotonic and well-behaved,\n",
    "            # it won't change anything.\n",
    "            scores = 1 / (1 + distances)\n",
    "            for i in range(len(distances)):\n",
    "                ap = average_precision_score(pid_matches[i], scores[i])\n",
    "\n",
    "                if np.isnan(ap):\n",
    "                    print()\n",
    "                    print(\"WARNING: encountered an AP of NaN!\")\n",
    "                    print(\"This usually means a person only appears once.\")\n",
    "                    print(\"In this case, it's because of {}.\".format(fids[i]))\n",
    "                    print(\"I'm excluding this person from eval and carrying on.\")\n",
    "                    print()\n",
    "                    continue\n",
    "\n",
    "                aps.append(ap)\n",
    "                # Find the first true match and increment the cmc data from there on.\n",
    "                k = np.where(pid_matches[i, np.argsort(distances[i])])[0][0]\n",
    "                cmc[k:] += 1\n",
    "\n",
    "                for j in range(len(dist)):\n",
    "                    ids = np.where(pid_matches[j, np.argsort(dist[j])])[0][0]\n",
    "                    acc[ids:] += 1\n",
    "\n",
    "    # Compute the actual cmc and mAP values\n",
    "    cmc = cmc / len(query_pids)\n",
    "    acc = acc / len(query_pids)\n",
    "    mean_ap = np.mean(aps)\n",
    "\n",
    "    # Save important data\n",
    "    if args.filename is not None:\n",
    "        json.dump({'mAP': mean_ap, 'CMC': list(cmc), 'aps': list(aps)}, args.filename)\n",
    "\n",
    "    # Print out a short summary.\n",
    "    print('mAP: {:.2%} | top-1: {:.2%} top-2: {:.2%} | top-5: {:.2%} | top-10: {:.2%} | acc: {:.2%}'.format(\n",
    "        mean_ap, cmc[0], cmc[1], cmc[4], cmc[9], acc[0]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'make_one_shot_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b028c8a97e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We go through the queries in batches, but we always need the whole gallery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mquery_pids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_fids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ).batch(args.batch_size).make_one_shot_iterator().get_next()\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'make_one_shot_iterator'"
     ]
    }
   ],
   "source": [
    "# We go through the queries in batches, but we always need the whole gallery\n",
    "batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices(\n",
    "    (query_pids, query_fids, query_embs)\n",
    ").batch(args.batch_size).make_one_shot_iterator().get_next()\n",
    "batch_distances = loss.cdist(batch_embs, gallery_embs, metric=args.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'make_one_shot_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ff3df03e223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We go through the queries in batches, but we always need the whole gallery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m batch_pids, batch_fids, batch_embs = tf.data.Dataset.from_tensor_slices(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mquery_pids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_fids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ).batch(args.batch_size).make_one_shot_iterator().get_next()\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'make_one_shot_iterator'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loop over the query embeddings and compute their APs and the CMC curve.\n",
    "aps = []\n",
    "cmc = np.zeros(len(gallery_pids), dtype=np.int32)\n",
    "with tf.Session() as sess:\n",
    "    for start_idx in count(step=args.batch_size):\n",
    "        try:\n",
    "            # Compute distance to all gallery embeddings\n",
    "            distances, pids, fids = sess.run([\n",
    "                batch_distances, batch_pids, batch_fids])\n",
    "            print('\\rEvaluating batch {}-{}/{}'.format(\n",
    "                    start_idx, start_idx + len(fids), len(query_fids)),\n",
    "                  flush=True, end='')\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print()  # Done!\n",
    "            break\n",
    "\n",
    "        # Convert the array of objects back to array of strings\n",
    "        pids, fids = np.array(pids, '|U'), np.array(fids, '|U')\n",
    "        #print(fids)\n",
    "        #plt.imshow(fids)\n",
    "        # Compute the pid matches\n",
    "        pid_matches = gallery_pids[None] == pids[:,None]\n",
    "        # Get a mask indicating True for those gallery entries that should\n",
    "        # be ignored for whatever reason (same camera, junk, ...) and\n",
    "        # exclude those in a way that doesn't affect CMC and mAP.\n",
    "        mask = excluder(fids)\n",
    "        distances[mask] = np.inf\n",
    "        pid_matches[mask] = False\n",
    "\n",
    "        # Keep track of statistics. Invert distances to scores using any\n",
    "        # arbitrary inversion, as long as it's monotonic and well-behaved,\n",
    "        # it won't change anything.\n",
    "        scores = 1 / (1 + distances)\n",
    "        for i in range(len(distances)):\n",
    "            ap = average_precision_score(pid_matches[i], scores[i])\n",
    "\n",
    "            if np.isnan(ap):\n",
    "                print()\n",
    "                print(\"WARNING: encountered an AP of NaN!\")\n",
    "                print(\"This usually means a person only appears once.\")\n",
    "                print(\"In this case, it's because of {}.\".format(fids[i]))\n",
    "                print(\"I'm excluding this person from eval and carrying on.\")\n",
    "                print()\n",
    "                continue\n",
    "\n",
    "            aps.append(ap)\n",
    "            print(aps)\n",
    "            # Find the first true match and increment the cmc data from there on.\n",
    "            k = np.where(pid_matches[i, np.argsort(distances[i])])[0][0]\n",
    "            cmc[k:] += 1\n",
    "\n",
    "# Compute the actual cmc and mAP values\n",
    "cmc = cmc / len(query_pids)\n",
    "print()\n",
    "mean_ap = np.mean(aps)\n",
    "\n",
    "# Save important data\n",
    "#if args.filename is not None:\n",
    "    #json.dump({'mAP': mean_ap, 'CMC': list(cmc), 'aps': list(aps)}, args.filename)\n",
    "\n",
    "# Print out a short summary.\n",
    "print('mAP: {:.2%} | top-1: {:.2%} top-2: {:.2%} | top-5: {:.2%} | top-10: {:.2%}'.format(\n",
    "    mean_ap, cmc[0], cmc[1], cmc[4], cmc[9]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_path, _ = image_datasets['query'].imgs[i]\n",
    "query_label = query_label[i]\n",
    "print(query_path)\n",
    "print('Top 10 images are as follow:')\n",
    "try: # Visualize Ranking Result \n",
    "    # Graphical User Interface is needed\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    ax = plt.subplot(1,11,1)\n",
    "    ax.axis('off')\n",
    "    imshow(query_path,'query')\n",
    "    for i in range(10):\n",
    "        ax = plt.subplot(1,11,i+2)\n",
    "        ax.axis('off')\n",
    "        img_path, _ = image_datasets['gallery'].imgs[index[i]]\n",
    "        label = gallery_label[index[i]]\n",
    "        imshow(img_path)\n",
    "        if label == query_label:\n",
    "            ax.set_title('%d'%(i+1), color='green')\n",
    "        else:\n",
    "            ax.set_title('%d'%(i+1), color='red')\n",
    "        print(img_path)\n",
    "except RuntimeError:\n",
    "    for i in range(10):\n",
    "        img_path = image_datasets.imgs[index[i]]\n",
    "        print(img_path[0])\n",
    "    print('If you want to see the visualization of the ranking result, graphical user interface is needed.')\n",
    "\n",
    "fig.savefig(\"show.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
