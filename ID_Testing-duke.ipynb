{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c590ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "from itertools import count\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import loss\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LeakyReLU, Concatenate, concatenate, Lambda, UpSampling2D, Add, Input, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b441c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    excluder= 'duke' #, 'diagonal','cuhk03','duke'\n",
    "    query_dataset = '/home/k/kajal/triplet-reid/data/duke/dukeMTMC_query.csv'\n",
    "    query_embeddings = '/home/k/kajal/triplet-reid/experiments/duke_mobile/aug_query_embeddings.h5'\n",
    "    #query_embeddings_adv = '/home/k/kajal/triplet-reid/experiments//MARKET/marketrecon255/test_embeddings.h5'\n",
    "    gallery_dataset = '/home/k/kajal/triplet-reid/data/duke/dukeMTMC_test.csv'\n",
    "    gallery_embeddings =  '/home/k/kajal/triplet-reid/experiments/duke_mobile/aug_gallery_embeddings.h5'\n",
    "    metric =  'euclidean'\n",
    "    #filename = \n",
    "    batch_size = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f340f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc995128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the query and gallery data from the CSV files.\n",
    "query_pids, query_fids = common.load_dataset(args.query_dataset, None)\n",
    "gallery_pids, gallery_fids = common.load_dataset(args.gallery_dataset, None)\n",
    "\n",
    "# Load the two datasets fully into memory.\n",
    "with h5py.File(args.query_embeddings, 'r') as f_query:\n",
    "    query_embs = np.array(f_query['emb'])\n",
    "    query_embs_dp = np.array(f_query['emb'])\n",
    "with h5py.File(args.gallery_embeddings, 'r') as f_gallery:\n",
    "    gallery_embs = np.array(f_gallery['emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ba7c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "66\n",
      "66\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "gallery_embs=gallery_embs[228:1583]\n",
    "gallery_pids=gallery_pids[228:1583]\n",
    "query_embs=query_embs[4:231]\n",
    "query_pids=query_pids[4:231]\n",
    "\n",
    "#gallery_embs=gallery_embs[228:13316]\n",
    "#gallery_pids=gallery_pids[228:13316]\n",
    "#gallery_embs=gallery_embs[228:16654]\n",
    "#gallery_pids=gallery_pids[228:16654]\n",
    "#gallery_embs=gallery_embs[8095:]\n",
    "#gallery_pids=gallery_pids[8095:]\n",
    "#gallery_pids=gallery_pids[185:201],gallery_pids[228:16654]\n",
    "uni_pids=np.unique(gallery_pids)\n",
    "print(len(uni_pids))\n",
    "uni_query_pids=np.unique(query_pids)\n",
    "print(len(uni_query_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1191a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_pids=np.unique(gallery_pids)\n",
    "print(len(uni_pids))\n",
    "uni_query_pids=np.unique(query_pids)\n",
    "len(uni_query_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e17ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 128)\n",
      "(1355, 128)\n",
      "(227, 128)\n",
      "(1355, 128)\n"
     ]
    }
   ],
   "source": [
    "print(query_embs.shape)\n",
    "print(gallery_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d48cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.0039964 -0.031202655 5.4245143 1.273047\n",
      "-5.0039964 -0.031202655 5.4245143 1.273047\n"
     ]
    }
   ],
   "source": [
    "#query_embs_dp=query_embs\n",
    "query_embs_dp.shape\n",
    "query_embs[0]\n",
    "print(np.min(query_embs),np.mean(query_embs),np.max(query_embs),np.std(query_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embs_dp[0,1]=np.random.normal(0.021068161,2.312213)\n",
    "query_embs_dp=np.exp(0.7) * query_embs_dp +0.001\n",
    "print(query_embs[0])\n",
    "print(query_embs_dp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.max(np.subtract(actual,pred)).mean()\n",
    "\n",
    "def mse(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.max(np.square(np.subtract(actual,pred)).mean())\n",
    "\n",
    "D0 = query_embs + np.random.normal(0.021068161,2.312213)\n",
    "D1 = query_embs_dp + np.random.normal(0.021068161,2.312213)\n",
    "#print(D0,D1)\n",
    "#sens1 = l1(query_embs,query_embs_dp)\n",
    "sens2 = mse(D0,D1)\n",
    "print(sens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8f38c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values ['0019' '0019' '0019' '0021' '0021' '0021' '0021' '0023' '0023' '0023'\n",
      " '0023' '0025' '0025' '0027' '0027' '0027' '0027' '0030' '0030' '0030'\n",
      " '0030' '0031' '0031' '0031' '0031' '0033' '0033' '0033' '0033' '0034'\n",
      " '0034' '0035' '0035' '0039' '0039' '0039' '0039' '0042' '0042' '0042'\n",
      " '0042' '0043' '0043' '0043' '0043' '0044' '0044' '0044' '0044' '0046'\n",
      " '0046' '0046' '0046' '0047' '0047' '0047' '0047' '0049' '0049' '0050'\n",
      " '0050' '0050' '0050' '0051' '0051' '0051' '0051' '0053' '0053' '0056'\n",
      " '0056' '0061' '0061' '0066' '0066' '0066' '0066' '0068' '0068' '0068'\n",
      " '0068' '0069' '0069' '0069' '0072' '0072' '0072' '0072' '0075' '0075'\n",
      " '0075' '0075' '0076' '0076' '0076' '0076' '0077' '0077' '0077' '0078'\n",
      " '0078' '0078' '0078' '0079' '0079' '0080' '0080' '0080' '0080' '0083'\n",
      " '0083' '0083' '0083' '0086' '0086' '0086' '0086' '0088' '0088' '0088'\n",
      " '0088' '0089' '0089' '0090' '0090' '0091' '0091' '0092' '0092' '0095'\n",
      " '0095' '0095' '0095' '0097' '0097' '0098' '0098' '0098' '0098' '0099'\n",
      " '0099' '0099' '0099' '0101' '0101' '0103' '0103' '0103' '0103' '0106'\n",
      " '0106' '0106' '0106' '0107' '0107' '0107' '0107' '0109' '0109' '0109'\n",
      " '0109' '0111' '0111' '0111' '0111' '0112' '0112' '0112' '0112' '0114'\n",
      " '0114' '0114' '0114' '0115' '0115' '0117' '0117' '0117' '0117' '0118'\n",
      " '0118' '0118' '0118' '0119' '0119' '0119' '0119' '0119' '0122' '0122'\n",
      " '0123' '0123' '0123' '0123' '0125' '0125' '0126' '0126' '0126' '0126'\n",
      " '0126' '0127' '0127' '0128' '0128' '0128' '0128' '0134' '0134' '0134'\n",
      " '0134' '0134' '0135' '0135' '0135' '0135' '0136' '0136' '0136' '0137'\n",
      " '0137' '0137' '0137' '0137' '0140' '0140' '0140']\n",
      "encoded [ 0  0  0  1  1  1  1  2  2  2  2  3  3  4  4  4  4  5  5  5  5  6  6  6\n",
      "  6  7  7  7  7  8  8  9  9 10 10 10 10 11 11 11 11 12 12 12 12 13 13 13\n",
      " 13 14 14 14 14 15 15 15 15 16 16 17 17 17 17 18 18 18 18 19 19 20 20 21\n",
      " 21 22 22 22 22 23 23 23 23 24 24 24 25 25 25 25 26 26 26 26 27 27 27 27\n",
      " 28 28 28 29 29 29 29 30 30 31 31 31 31 32 32 32 32 33 33 33 33 34 34 34\n",
      " 34 35 35 36 36 37 37 38 38 39 39 39 39 40 40 41 41 41 41 42 42 42 42 43\n",
      " 43 44 44 44 44 45 45 45 45 46 46 46 46 47 47 47 47 48 48 48 48 49 49 49\n",
      " 49 50 50 50 50 51 51 52 52 52 52 53 53 53 53 54 54 54 54 54 55 55 56 56\n",
      " 56 56 57 57 58 58 58 58 58 59 59 60 60 60 60 61 61 61 61 61 62 62 62 62\n",
      " 63 63 63 64 64 64 64 64 65 65 65]\n",
      "(227, 66)\n",
      "values ['0019' '0019' '0019' '0021' '0021' '0021' '0021' '0023' '0023' '0023'\n",
      " '0023' '0025' '0025' '0027' '0027' '0027' '0027' '0030' '0030' '0030'\n",
      " '0030' '0031' '0031' '0031' '0031' '0033' '0033' '0033' '0033' '0034'\n",
      " '0034' '0035' '0035' '0039' '0039' '0039' '0039' '0042' '0042' '0042'\n",
      " '0042' '0043' '0043' '0043' '0043' '0044' '0044' '0044' '0044' '0046'\n",
      " '0046' '0046' '0046' '0047' '0047' '0047' '0047' '0049' '0049' '0050'\n",
      " '0050' '0050' '0050' '0051' '0051' '0051' '0051' '0053' '0053' '0056'\n",
      " '0056' '0061' '0061' '0066' '0066' '0066' '0066' '0068' '0068' '0068'\n",
      " '0068' '0069' '0069' '0069' '0072' '0072' '0072' '0072' '0075' '0075'\n",
      " '0075' '0075' '0076' '0076' '0076' '0076' '0077' '0077' '0077' '0078'\n",
      " '0078' '0078' '0078' '0079' '0079' '0080' '0080' '0080' '0080' '0083'\n",
      " '0083' '0083' '0083' '0086' '0086' '0086' '0086' '0088' '0088' '0088'\n",
      " '0088' '0089' '0089' '0090' '0090' '0091' '0091' '0092' '0092' '0095'\n",
      " '0095' '0095' '0095' '0097' '0097' '0098' '0098' '0098' '0098' '0099'\n",
      " '0099' '0099' '0099' '0101' '0101' '0103' '0103' '0103' '0103' '0106'\n",
      " '0106' '0106' '0106' '0107' '0107' '0107' '0107' '0109' '0109' '0109'\n",
      " '0109' '0111' '0111' '0111' '0111' '0112' '0112' '0112' '0112' '0114'\n",
      " '0114' '0114' '0114' '0115' '0115' '0117' '0117' '0117' '0117' '0118'\n",
      " '0118' '0118' '0118' '0119' '0119' '0119' '0119' '0119' '0122' '0122'\n",
      " '0123' '0123' '0123' '0123' '0125' '0125' '0126' '0126' '0126' '0126'\n",
      " '0126' '0127' '0127' '0128' '0128' '0128' '0128' '0134' '0134' '0134'\n",
      " '0134' '0134' '0135' '0135' '0135' '0135' '0136' '0136' '0136' '0137'\n",
      " '0137' '0137' '0137' '0137' '0140' '0140' '0140']\n",
      "encoded [ 0  0  0  1  1  1  1  2  2  2  2  3  3  4  4  4  4  5  5  5  5  6  6  6\n",
      "  6  7  7  7  7  8  8  9  9 10 10 10 10 11 11 11 11 12 12 12 12 13 13 13\n",
      " 13 14 14 14 14 15 15 15 15 16 16 17 17 17 17 18 18 18 18 19 19 20 20 21\n",
      " 21 22 22 22 22 23 23 23 23 24 24 24 25 25 25 25 26 26 26 26 27 27 27 27\n",
      " 28 28 28 29 29 29 29 30 30 31 31 31 31 32 32 32 32 33 33 33 33 34 34 34\n",
      " 34 35 35 36 36 37 37 38 38 39 39 39 39 40 40 41 41 41 41 42 42 42 42 43\n",
      " 43 44 44 44 44 45 45 45 45 46 46 46 46 47 47 47 47 48 48 48 48 49 49 49\n",
      " 49 50 50 50 50 51 51 52 52 52 52 53 53 53 53 54 54 54 54 54 55 55 56 56\n",
      " 56 56 57 57 58 58 58 58 58 59 59 60 60 60 60 61 61 61 61 61 62 62 62 62\n",
      " 63 63 63 64 64 64 64 64 65 65 65]\n",
      "(227, 66)\n"
     ]
    }
   ],
   "source": [
    "query_data = query_pids\n",
    "query_values = array(query_data)\n",
    "print('values', query_values)\n",
    "# integer encode\n",
    "query_label_encoder = LabelEncoder()\n",
    "query_integer_encoded = query_label_encoder.fit_transform(query_values)\n",
    "print('encoded', query_integer_encoded)\n",
    "# binary encode\n",
    "query_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "query_integer_encoded = query_integer_encoded.reshape(len(query_integer_encoded), 1)\n",
    "query_onehot_encoded = query_onehot_encoder.fit_transform(query_integer_encoded)\n",
    "print(query_onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e8f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values ['0019' '0019' '0019' ... '0140' '0140' '0140']\n",
      "encoded [ 0  0  0 ... 65 65 65]\n",
      "(1355, 66)\n",
      "values ['0019' '0019' '0019' ... '0140' '0140' '0140']\n",
      "encoded [ 0  0  0 ... 65 65 65]\n",
      "(1355, 66)\n"
     ]
    }
   ],
   "source": [
    "data = gallery_pids\n",
    "values = array(data)\n",
    "print('values', values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print('encoded', integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5569cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = gallery_embs\n",
    "Y_train_label = onehot_encoded\n",
    "X_test = query_embs\n",
    "Y_test_label = query_onehot_encoded\n",
    "#print(X_train.size)\n",
    "#print(X_test.size)\n",
    "#print(Y_train_label)\n",
    "#print(Y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ad0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Y_train_label[2:5000])\n",
    "#y_train_label = to_categorical(Y_train_label)\n",
    "#y_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c5e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      " dense (Dense)               (None, 66)                8514      \n",
      " dense (Dense)               (None, 66)                8514      \n",
      "                                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 8,514\n",
      "Total params: 8,514\n",
      "Trainable params: 8,514\n",
      "Non-trainable params: 0\n",
      "Trainable params: 8,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "#model.add( Dense(units=512, input_dim=128, kernel_initializer='normal', activation='relu') )\n",
    "#model.add( Dropout(0.5))\n",
    "model.add( Dense(units=66, input_dim=128, kernel_initializer='normal', activation='softmax') )\n",
    "print( model.summary() )\n",
    "model.compile( loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe9922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "9/9 - 3s - loss: 4.1931 - accuracy: 0.0286 - val_loss: 4.2603 - val_accuracy: 0.0148 - 3s/epoch - 319ms/step\n",
      "9/9 - 3s - loss: 4.1931 - accuracy: 0.0286 - val_loss: 4.2603 - val_accuracy: 0.0148 - 3s/epoch - 319ms/step\n",
      "Epoch 2/3\n",
      "Epoch 2/3\n",
      "9/9 - 0s - loss: 3.4310 - accuracy: 0.2159 - val_loss: 4.4805 - val_accuracy: 0.0037 - 151ms/epoch - 17ms/step\n",
      "9/9 - 0s - loss: 3.4310 - accuracy: 0.2159 - val_loss: 4.4805 - val_accuracy: 0.0037 - 151ms/epoch - 17ms/step\n",
      "Epoch 3/3\n",
      "Epoch 3/3\n",
      "9/9 - 0s - loss: 2.7778 - accuracy: 0.4963 - val_loss: 4.7146 - val_accuracy: 0.0037 - 210ms/epoch - 23ms/step\n",
      "9/9 - 0s - loss: 2.7778 - accuracy: 0.4963 - val_loss: 4.7146 - val_accuracy: 0.0037 - 210ms/epoch - 23ms/step\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit( x=gallery_embs, y=Y_train_label, validation_split=0.2, epochs=3, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89cc86fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 2.5442 - accuracy: 0.4062Evaluate on test data\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 2.8761 - accuracy: 0.4709\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 2.8761 - accuracy: 0.4709\n",
      "test loss, test acc: [2.8760554790496826, 0.4708520174026489]\n",
      "Generate predictions for 3 samples\n",
      "test loss, test acc: [2.8760554790496826, 0.4708520174026489]\n",
      "Generate predictions for 3 samples\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "predictions shape: [[0.06696355 0.00524317 0.00837805 ... 0.00411428 0.00324478 0.0008084 ]\n",
      " [0.06880886 0.00929948 0.02813125 ... 0.00433414 0.00664374 0.00257625]\n",
      " [0.04402748 0.0061026  0.0121971  ... 0.00657656 0.00696013 0.00242293]\n",
      " ...\n",
      " [0.01580173 0.05368307 0.01131012 ... 0.00559588 0.00394196 0.00647921]\n",
      " [0.01707824 0.03140106 0.00501237 ... 0.01451706 0.01465894 0.02496673]\n",
      " [0.0114368  0.03312156 0.02134123 ... 0.02341807 0.01441503 0.0241951 ]]\n",
      "predictions shape: [[0.06696355 0.00524317 0.00837805 ... 0.00411428 0.00324478 0.0008084 ]\n",
      " [0.06880886 0.00929948 0.02813125 ... 0.00433414 0.00664374 0.00257625]\n",
      " [0.04402748 0.0061026  0.0121971  ... 0.00657656 0.00696013 0.00242293]\n",
      " ...\n",
      " [0.01580173 0.05368307 0.01131012 ... 0.00559588 0.00394196 0.00647921]\n",
      " [0.01707824 0.03140106 0.00501237 ... 0.01451706 0.01465894 0.02496673]\n",
      " [0.0114368  0.03312156 0.02134123 ... 0.02341807 0.01441503 0.0241951 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test[4:2081], Y_test_label[4:2081])\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(X_test)\n",
    "print(\"predictions shape:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffprivlib.models import GaussianNB\n",
    "clf = GaussianNB(epsilon=1,bounds=(-50e50, 50e50))\n",
    "clf.fit(X_train, gallery_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)\n",
    "print(\"Test accuracy: %f\" % clf.score(X_test, query_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cec2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hbeta(D=np.array([]), beta=1.0):\n",
    "    \"\"\"\n",
    "        Compute the perplexity and the P-row for a specific value of the\n",
    "        precision of a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = np.exp(-D.copy() * beta)\n",
    "    sumP = sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a binary search to get P-values in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    P = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    logU = np.log(perplexity)\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
    "        (H, thisP) = Hbeta(Di, beta[i])\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while np.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    beta[i] = beta[i] * 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2.\n",
    "            else:\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    beta[i] = beta[i] / 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2.\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
    "    return P\n",
    "\n",
    "def pca(X=np.array([]), no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
    "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
    "    Y = np.dot(X, M[:, 0:no_dims])\n",
    "    return Y\n",
    "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims).real\n",
    "    (n, d) = X.shape\n",
    "    max_iter = 10\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 200\n",
    "    min_gain = 0.01\n",
    "    Y = np.random.randn(n, no_dims)\n",
    "    dY = np.zeros((n, no_dims))\n",
    "    iY = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity)\n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "    \n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = np.sum(np.square(Y), 1)\n",
    "        num = -2. * np.dot(Y, Y.T)\n",
    "        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
    "        num[range(n), range(n)] = 0.\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
    "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iY = momentum * iY - eta * (gains * dY)\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            C = np.sum(P * np.log(P / Q))\n",
    "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4.\n",
    "\n",
    "    # Return solution\n",
    "    return Y\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb470a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "X = gallery_embs[500:700]\n",
    "lbl=list(map(int,gallery_pids))\n",
    "labels = lbl[500:700]#query_pids[0:20]\n",
    "pylab.figure()\n",
    "Y = tsne(X, 2, 50, 20.0)\n",
    "plt=pylab.scatter(Y[:, 0],Y[:,1],10,labels,marker='o')\n",
    "pylab.axis('off')\n",
    "pylab.savefig('rsltn3n1.pdf')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test_label)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(X_test)\n",
    "print(\"predictions shape:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed85944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "X = query_embs[500:700]\n",
    "lbl=list(map(int,query_pids))\n",
    "labels = lbl[500:700]#query_pids[0:20]\n",
    "pylab.figure()\n",
    "Y = tsne(X, 2, 50, 20.0)\n",
    "plt=pylab.scatter(Y[:, 0],Y[:,1],10,labels,marker='o')\n",
    "pylab.axis('off')\n",
    "pylab.savefig('rsltn3n2.pdf')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #file = query_embs\n",
    "    #filel = query_pids\n",
    "    X = query_embs[0:50]\n",
    "    lbl=list(map(int,query_pids))\n",
    "    labels = lbl[0:50]#query_pids[0:20]\n",
    "    print(query_embs,query_pids)\n",
    "    #X = np.loadtxt(\"mnist2500_X.txt\")\n",
    "    pylab.figure()\n",
    "    Y = tsne(X, 2, 50, 20.0)\n",
    "    plt=pylab.scatter(Y[:, 0],Y[:,1],30,labels,marker='+')\n",
    "    pylab.axis('off')\n",
    "    pylab.legend('on')\n",
    "    pylab.savefig('rsltn3n1.pdf')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #file = query_embs\n",
    "    #filel = query_pids\n",
    "    X = query_embs[0:50]\n",
    "    lbl=list(map(int,query_pids))\n",
    "    labels = lbl[0:50]#query_pids[0:20]\n",
    "    print(query_embs,query_pids)\n",
    "    #X = np.loadtxt(\"mnist2500_X.txt\")\n",
    "    pylab.figure()\n",
    "    Y = tsne(X, 2, 50, 20.0)\n",
    "    plt=pylab.scatter(Y[:, 0],Y[:,1],30,labels,marker='+')\n",
    "    pylab.axis('off')\n",
    "    pylab.legend('on')\n",
    "    pylab.savefig('rsltn3n1.pdf')\n",
    "    pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
