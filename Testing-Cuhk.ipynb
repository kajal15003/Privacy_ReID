{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c590ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "from itertools import count\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "import tensorflow as tf\n",
    "\n",
    "import commoncuhk\n",
    "import loss\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LeakyReLU, Concatenate, concatenate, Lambda, UpSampling2D, Add, Input, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "python cuhk-evaluate.py --excluder cuhk03 \n",
    "--query_dataset data/cuhk03-np/detected_query.csv \n",
    "--query_embeddings experiments/cuhk1rec/query_embeddings.h5 \n",
    "--gallery_dataset data/cuhk03-np/detected_test.csv \n",
    "--gallery_embeddings experiments/cuhk1rec/gallery_embeddings.h5 --metric euclidean --batch_size 18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b441c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    excluder= 'market1501' #, 'diagonal','cuhk03','duke'\n",
    "    query_dataset = '/home/k/kajal/triplet-reid/data/cuhk03-np/detected_query.csv'\n",
    "    query_embeddings = '/home/k/kajal/triplet-reid/experiments/cuhk_res101/query_embeddings.h5'\n",
    "    query_embeddings_adv = '/home/k/kajal/triplet-reid/experiments/cuhk1rec/query_embeddings.h5'\n",
    "    gallery_dataset = '/home/k/kajal/triplet-reid/data/cuhk03-np/detected_test.csv'\n",
    "    gallery_embeddings =  '/home/k/kajal/triplet-reid/experiments/cuhk_res101/gallery_embeddings.h5'\n",
    "    metric =  'euclidean'\n",
    "    #filename = \n",
    "    batch_size = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f340f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc995128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the query and gallery data from the CSV files.\n",
    "query_pids, query_fids = commoncuhk.load_dataset(args.query_dataset, None)\n",
    "gallery_pids, gallery_fids = commoncuhk.load_dataset(args.gallery_dataset, None)\n",
    "\n",
    "# Load the two datasets fully into memory.\n",
    "with h5py.File(args.query_embeddings, 'r') as f_query:\n",
    "    query_embs = np.array(f_query['emb'])\n",
    "with h5py.File(args.gallery_embeddings, 'r') as f_gallery:\n",
    "    gallery_embs = np.array(f_gallery['emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d241b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_embs=gallery_embs\n",
    "gallery_pids=gallery_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1191a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_pids=np.unique(gallery_pids)\n",
    "print(len(uni_pids))\n",
    "uni_query_pids=np.unique(query_pids)\n",
    "len(uni_query_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e17ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 128)\n",
      "(5332, 128)\n"
     ]
    }
   ],
   "source": [
    "print(query_embs.shape)\n",
    "print(gallery_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8f38c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values ['722' '103' '156' ... '369' '374' '1107']\n",
      "encoded [538  18 234 ... 351 354  53]\n",
      "(1400, 700)\n"
     ]
    }
   ],
   "source": [
    "query_data = query_pids\n",
    "query_values = array(query_data)\n",
    "print('values', query_values)\n",
    "# integer encode\n",
    "query_label_encoder = LabelEncoder()\n",
    "query_integer_encoded = query_label_encoder.fit_transform(query_values)\n",
    "print('encoded', query_integer_encoded)\n",
    "# binary encode\n",
    "query_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "query_integer_encoded = query_integer_encoded.reshape(len(query_integer_encoded), 1)\n",
    "query_onehot_encoded = query_onehot_encoder.fit_transform(query_integer_encoded)\n",
    "print(query_onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e8f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values ['136' '704' '303' ... '1111' '802' '1199']\n",
      "encoded [177 527 314 ...  56 586 101]\n",
      "(5332, 700)\n"
     ]
    }
   ],
   "source": [
    "data = gallery_pids\n",
    "values = array(data)\n",
    "print('values', values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print('encoded', integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b953826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = gallery_embs\n",
    "Y_train_label = onehot_encoded\n",
    "X_test = query_embs\n",
    "Y_test_label = query_onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c5e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 700)               90300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,300\n",
      "Trainable params: 90,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "#model.add( Dense(units=128, input_dim=128, kernel_initializer='normal', activation='relu') )\n",
    "#model.add( Dropout(0.5))\n",
    "model.add( Dense(units=700, input_dim=128, kernel_initializer='normal', activation='softmax') )\n",
    "print( model.summary() )\n",
    "model.compile( loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fe9922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "34/34 - 0s - loss: 0.2185 - accuracy: 0.9988 - val_loss: 1.6067 - val_accuracy: 0.6729 - 496ms/epoch - 15ms/step\n",
      "Epoch 2/4\n",
      "34/34 - 0s - loss: 0.2083 - accuracy: 0.9988 - val_loss: 1.6031 - val_accuracy: 0.6710 - 448ms/epoch - 13ms/step\n",
      "Epoch 3/4\n",
      "34/34 - 0s - loss: 0.1987 - accuracy: 0.9991 - val_loss: 1.5950 - val_accuracy: 0.6673 - 418ms/epoch - 12ms/step\n",
      "Epoch 4/4\n",
      "34/34 - 0s - loss: 0.1903 - accuracy: 0.9988 - val_loss: 1.5862 - val_accuracy: 0.6673 - 379ms/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(x=gallery_embs, y=Y_train_label, validation_split=0.2, epochs=4, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9372e1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.7076 - accuracy: 0.8829\n",
      "test loss, test acc: [0.7076228857040405, 0.8828571438789368]\n",
      "Generate predictions for 3 samples\n",
      "44/44 [==============================] - 0s 3ms/step\n",
      "predictions shape: [[2.6948990e-02 5.2166570e-07 3.8133643e-09 ... 3.4685070e-07\n",
      "  6.9019961e-07 5.7114262e-06]\n",
      " [6.3826032e-05 3.3079999e-05 1.3508439e-08 ... 1.1177374e-07\n",
      "  3.0135901e-08 2.2582599e-05]\n",
      " [3.1053105e-03 3.1782231e-06 6.6372266e-08 ... 1.2219591e-06\n",
      "  9.4686925e-07 2.0365996e-04]\n",
      " ...\n",
      " [1.6040160e-05 1.9363355e-07 3.2714297e-06 ... 3.7870146e-04\n",
      "  8.1811057e-05 1.0632847e-04]\n",
      " [9.8227232e-05 4.4606049e-06 1.0474688e-08 ... 2.5859282e-07\n",
      "  8.4168488e-07 3.6592743e-07]\n",
      " [2.3667778e-06 9.3013060e-04 4.1372183e-05 ... 5.2756146e-05\n",
      "  2.2995553e-05 3.2336355e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test_label)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(X_test)\n",
    "print(\"predictions shape:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hbeta(D=np.array([]), beta=1.0):\n",
    "    \"\"\"\n",
    "        Compute the perplexity and the P-row for a specific value of the\n",
    "        precision of a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = np.exp(-D.copy() * beta)\n",
    "    sumP = sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a binary search to get P-values in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    P = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    logU = np.log(perplexity)\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
    "        (H, thisP) = Hbeta(Di, beta[i])\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while np.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    beta[i] = beta[i] * 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2.\n",
    "            else:\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    beta[i] = beta[i] / 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2.\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
    "    return P\n",
    "\n",
    "def pca(X=np.array([]), no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
    "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
    "    Y = np.dot(X, M[:, 0:no_dims])\n",
    "    return Y\n",
    "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims).real\n",
    "    (n, d) = X.shape\n",
    "    max_iter = 10\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 200\n",
    "    min_gain = 0.01\n",
    "    Y = np.random.randn(n, no_dims)\n",
    "    dY = np.zeros((n, no_dims))\n",
    "    iY = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity)\n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "    \n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = np.sum(np.square(Y), 1)\n",
    "        num = -2. * np.dot(Y, Y.T)\n",
    "        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
    "        num[range(n), range(n)] = 0.\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
    "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iY = momentum * iY - eta * (gains * dY)\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            C = np.sum(P * np.log(P / Q))\n",
    "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4.\n",
    "\n",
    "    # Return solution\n",
    "    return Y\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "X = gallery_embs[500:700]\n",
    "lbl=list(map(int,gallery_pids))\n",
    "labels = lbl[500:700]#query_pids[0:20]\n",
    "pylab.figure()\n",
    "Y = tsne(X, 2, 50, 20.0)\n",
    "plt=pylab.scatter(Y[:, 0],Y[:,1],10,labels,marker='o')\n",
    "pylab.axis('off')\n",
    "pylab.savefig('rsltn3n2.jpg')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "X = query_embs[500:700]\n",
    "lbl=list(map(int,query_pids))\n",
    "labels = lbl[500:700]#query_pids[0:20]\n",
    "pylab.figure()\n",
    "Y = tsne(X, 2, 50, 20.0)\n",
    "plt=pylab.scatter(Y[:, 0],Y[:,1],10,labels,marker='o')\n",
    "pylab.axis('off')\n",
    "pylab.savefig('rsltn3n1.jpg')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffprivlib.models import GaussianNB\n",
    "clf = GaussianNB(epsilon=0.01)\n",
    "clf.fit(X_test, query_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy: %f\" % clf.score(X_test, query_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epsilons = np.logspace(-2, 2, 50)\n",
    "#bounds = ([4.3, 2.0, 1.1, 0.1], [7.9, 4.4, 6.9, 2.5])\n",
    "bounds=(np.array(128),np.array(128))\n",
    "accuracy = list()\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    clf = GaussianNB(bounds=bounds, epsilon=epsilon)\n",
    "    clf.fit(X_train, gallery_pids)\n",
    "    \n",
    "    accuracy.append(clf.score(X_test, query_pids))\n",
    "\n",
    "plt.semilogx(epsilons, accuracy)\n",
    "plt.title(\"Differentially private Naive Bayes accuracy\")\n",
    "plt.xlabel(\"epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0611285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffprivlib.models as dp\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "dp_clf = dp.LogisticRegression()\n",
    "dp_clf.fit(X_train, gallery_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Differentially private test accuracy (epsilon=%.2f): %.2f%%\" % \n",
    "     (dp_clf.epsilon, dp_clf.score(X_test, query_pids) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from diffprivlib import models\n",
    "pipe = Pipeline([\n",
    "    ('scaler', sk.preprocessing.StandardScaler()),\n",
    "    ('pca', sk.decomposition.PCA(2)),\n",
    "    ('lr', sk.linear_model.LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "pipe.fit(X_train, gallery_pids)\n",
    "baseline = pipe.score(X_test, query_pids)\n",
    "print(\"Non-private test accuracy: %.2f%%\" % (baseline * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_pipe = Pipeline([\n",
    "    ('scaler', models.StandardScaler(bounds=(np.array(128), np.array(128)))),\n",
    "    ('pca', models.PCA(2, data_norm=5, centered=True)),\n",
    "    ('lr', models.LogisticRegression(data_norm=5))\n",
    "])\n",
    "\n",
    "dp_pipe.fit(X_train, gallery_pids)\n",
    "print(\"Differentially private pipeline accuracy (epsilon=3): %.2f%%\" % (dp_pipe.score(X_test, query_pids) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a37027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
