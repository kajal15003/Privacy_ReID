{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879a2e86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7acb09745437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmkdir_if_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrite_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ..utils.data import Dataset\n",
    "from ..utils.osutils import mkdir_if_missing\n",
    "from ..utils.serialization import write_json\n",
    "\n",
    "\n",
    "class CUHK03(Dataset):\n",
    "    url = 'https://docs.google.com/spreadsheet/viewform?usp=drive_web&formkey=dHRkMkFVSUFvbTJIRkRDLWRwZWpONnc6MA#gid=0'\n",
    "    md5 = '728939e58ad9f0ff53e521857dd8fb43'\n",
    "\n",
    "    def __init__(self, root, split_id=0, num_val=100, download=True):\n",
    "        super(CUHK03, self).__init__(root, split_id=split_id)\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\"Dataset not found or corrupted. \" +\n",
    "                               \"You can use download=True to download it.\")\n",
    "\n",
    "        self.load(num_val)\n",
    "\n",
    "    def download(self):\n",
    "        if self._check_integrity():\n",
    "            print(\"Files already downloaded and verified\")\n",
    "            return\n",
    "\n",
    "        import h5py\n",
    "        import hashlib\n",
    "        from scipy.misc import imsave\n",
    "        from zipfile import ZipFile\n",
    "\n",
    "        raw_dir = osp.join(self.root, 'raw')\n",
    "        mkdir_if_missing(raw_dir)\n",
    "\n",
    "        # Download the raw zip file\n",
    "        fpath = osp.join(raw_dir, 'cuhk03_release.zip')\n",
    "        if osp.isfile(fpath) and \\\n",
    "          hashlib.md5(open(fpath, 'rb').read()).hexdigest() == self.md5:\n",
    "            print(\"Using downloaded file: \" + fpath)\n",
    "        else:\n",
    "            raise RuntimeError(\"Please download the dataset manually from {} \"\n",
    "                               \"to {}\".format(self.url, fpath))\n",
    "\n",
    "        # Extract the file\n",
    "        exdir = osp.join(raw_dir, 'cuhk03_release')\n",
    "        if not osp.isdir(exdir):\n",
    "            print(\"Extracting zip file\")\n",
    "            with ZipFile(fpath) as z:\n",
    "                z.extractall(path=raw_dir)\n",
    "\n",
    "        # Format\n",
    "        images_dir = osp.join(self.root, 'images')\n",
    "        mkdir_if_missing(images_dir)\n",
    "        matdata = h5py.File(osp.join(exdir, 'cuhk-03.mat'), 'r')\n",
    "\n",
    "        def deref(ref):\n",
    "            return matdata[ref][:].T\n",
    "\n",
    "        def dump_(refs, pid, cam, fnames):\n",
    "            for ref in refs:\n",
    "                img = deref(ref)\n",
    "                if img.size == 0 or img.ndim < 2: break\n",
    "                fname = '{:08d}_{:02d}_{:04d}.jpg'.format(pid, cam, len(fnames))\n",
    "                imsave(osp.join(images_dir, fname), img)\n",
    "                fnames.append(fname)\n",
    "\n",
    "        identities = []\n",
    "        for labeled, detected in zip(\n",
    "                matdata['labeled'][0], matdata['detected'][0]):\n",
    "            labeled, detected = deref(labeled), deref(detected)\n",
    "            assert labeled.shape == detected.shape\n",
    "            for i in range(labeled.shape[0]):\n",
    "                pid = len(identities)\n",
    "                images = [[], []]\n",
    "                dump_(labeled[i, :5], pid, 0, images[0])\n",
    "                dump_(detected[i, :5], pid, 0, images[0])\n",
    "                dump_(labeled[i, 5:], pid, 1, images[1])\n",
    "                dump_(detected[i, 5:], pid, 1, images[1])\n",
    "                identities.append(images)\n",
    "\n",
    "        # Save meta information into a json file\n",
    "        meta = {'name': 'cuhk03', 'shot': 'multiple', 'num_cameras': 2,\n",
    "                'identities': identities}\n",
    "        write_json(meta, osp.join(self.root, 'meta.json'))\n",
    "\n",
    "        # Save training and test splits\n",
    "        splits = []\n",
    "        view_counts = [deref(ref).shape[0] for ref in matdata['labeled'][0]]\n",
    "        vid_offsets = np.r_[0, np.cumsum(view_counts)]\n",
    "        for ref in matdata['testsets'][0]:\n",
    "            test_info = deref(ref).astype(np.int32)\n",
    "            test_pids = sorted(\n",
    "                [int(vid_offsets[i-1] + j - 1) for i, j in test_info])\n",
    "            trainval_pids = list(set(range(vid_offsets[-1])) - set(test_pids))\n",
    "            split = {'trainval': trainval_pids,\n",
    "                     'query': test_pids,\n",
    "                     'gallery': test_pids}\n",
    "            splits.append(split)\n",
    "        write_json(splits, osp.join(self.root, 'splits.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78558784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
